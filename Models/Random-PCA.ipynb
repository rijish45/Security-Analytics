{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis and Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to perform PCA and Random Forest to provide insight into our\n",
    "full data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some packages and functions that will be used throughout the notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def most_common(row):\n",
    "    return max(set(row), key=row.count)\n",
    "\n",
    "def get_seconds(row):\n",
    "    return row.second\n",
    "\n",
    "def sort_dt(row):\n",
    "    return sorted(row)\n",
    "\n",
    "def get_td_mean(row):\n",
    "    td = 0\n",
    "    if len(row) > 2:\n",
    "        for i in range (0,len(row)-1):\n",
    "            td += row[i+1]-row[i]\n",
    "        return td/(len(row)-1)\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "def get_td_sd(row):\n",
    "    sd = 0\n",
    "    new_list = []\n",
    "    if len(row) > 2:\n",
    "        for i in range (0,len(row)-1):\n",
    "            new_list.append(row[i+1]-row[i])\n",
    "        return statistics.stdev(new_list)\n",
    "    else:\n",
    "        return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import original data file from 23rd of May 2019\n",
    "\n",
    "data = pd.read_csv(\"/Users/siboraseranaj/Downloads/stingar_full-20190523(2).csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we engineered features based on the variables we already had. Most of our data was string data, so it was neccessary step. \n",
    "The features we created were:\n",
    "\n",
    "    1. mean_time_difference which shows the mean difference in time between two consecutive          attacks from the same IP\n",
    "    2. sd_time_difference which shows the standard deviation in time difference between two          consecutive attacks from the same ip\n",
    "    3. length_username which shows the length of the username input by the attacker\n",
    "    4. length_password which shows the length of the password input by the attacker\n",
    "    5. sensor_number which shows the number of sensors hit by the attacker \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['d_time'] = pd.to_datetime(data['d_time']).values.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature mean_time_difference which shows the mean difference in time between two consecutive attacks from the same ip\n",
    "\n",
    "data_grouped_td = data[['src_ip','d_time']].groupby('src_ip',as_index = False).agg({'d_time':lambda x: x.tolist()})\n",
    "data_grouped_td['d_time'] = data_grouped_td['d_time'].apply(sort_dt)\n",
    "data_grouped_td['d_time'] = data_grouped_td['d_time'].apply(get_td_mean)\n",
    "data_grouped_td['d_time'] = pd.to_datetime(data_grouped_td['d_time'], unit='ns')\n",
    "data_grouped_td['d_time'] = data_grouped_td['d_time'].apply(get_seconds)\n",
    "data_grouped_td = data_grouped_td.rename({'d_time':'mean_time_difference'},axis = 1)\n",
    "data_grouped_td.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create feature sd_time_difference which shows the standard deviation in time difference between two consecutive attacks from the same ip\n",
    "\n",
    "data_grouped_sd = data[['src_ip','d_time']].groupby('src_ip',as_index = False).agg({'d_time':lambda x: x.tolist()})\n",
    "data_grouped_sd['d_time'] = data_grouped_sd['d_time'].apply(sort_dt)\n",
    "data_grouped_sd['d_time'] = data_grouped_sd['d_time'].apply(get_td_sd)\n",
    "data_grouped_sd['d_time'] = pd.to_datetime(data_grouped_sd['d_time'], unit='ns')\n",
    "data_grouped_sd['d_time'] = data_grouped_sd['d_time'].apply(get_seconds)\n",
    "data_grouped_sd = data_grouped_sd.rename({'d_time':'sd_time_difference'},axis = 1)\n",
    "data_grouped_sd.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_grouped = data[['src_ip','d_time']].groupby('src_ip',as_index = False).agg({'d_time':np.mean})\n",
    "data_grouped['d_time'] = pd.to_datetime(data_grouped['d_time'], unit='ns')\n",
    "data_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_grouped1 = data[['src_ip','d_time']].groupby('src_ip',as_index = False).agg({'d_time':np.std})\n",
    "data_grouped1['d_time'] = pd.to_datetime(data_grouped1['d_time'], unit='ns')\n",
    "data_grouped1['d_time'] = data_grouped1['d_time'].apply(get_seconds)\n",
    "data_grouped1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of sensor hit by the attacker \n",
    "def rem_dups(array):\n",
    "    myset = set(array)\n",
    "    return len(list(myset))\n",
    "\n",
    "data_grouped2 = data[['sensor','src_ip']].groupby('src_ip',as_index = False).agg({'sensor':lambda x: x.tolist()})\n",
    "data_grouped2['sensor_number'] = data_grouped2['sensor'].apply(rem_dups)\n",
    "data_grouped2['sensor_number'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = data_grouped.merge(data_grouped1, left_on='src_ip', right_on='src_ip')\n",
    "data_final = data_final.merge(data_grouped_td,left_on='src_ip', right_on='src_ip')\n",
    "data_final = data_final.merge(data_grouped_sd,left_on='src_ip', right_on='src_ip')\n",
    "data_final = data_final.merge(data_grouped2,left_on='src_ip', right_on='src_ip')\n",
    "data_final = data_final.rename({'d_time_x':'mean_time_of_attack','d_time_y':'sd_time_of_attack','sensor':'all_sensors'},axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.drop([\"mean_time_of_attack\", \"sd_time_of_attack\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.drop([\"mean_time_of_attack\", \"sd_time_of_attack\", \"all_sensors\"], axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = data[[\"ssh_username\", \"src_ip\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new[\"length_username\"] = new[\"ssh_username\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_length = new.groupby(\"src_ip\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = pd.merge(user_length, data_final, how = \"outer\", on = \"src_ip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current['length_username'].fillna(value = current['length_username'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_command = data[['src_ip', 'command']]\n",
    "new_command.dropna(inplace = True)\n",
    "new_command['length_command'] = new_command['command'].apply(len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = new_command.groupby('src_ip').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = pd.merge(current, feature, how = 'outer', on = \"src_ip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current['length_command'].fillna(value = current['length_command'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['app'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = data['app'].value_counts()\n",
    "res = data[~data['app'].isin(counts[counts < 27].index)]\n",
    "res['app'].value_counts()\n",
    "honeypot = res[['app', 'src_ip']]\n",
    "honeypot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "honeypot['app'].value_counts()\n",
    "honeypot.groupby('src_ip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_current = pd.merge(current, honeypot, how = 'inner', on = 'src_ip')\n",
    "new_current.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = data.groupby('src_ip')[['src_ip', 'app']].head()\n",
    "temp = dat.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let = pd.get_dummies(temp['app'])\n",
    "let.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det = pd.concat([let,temp], axis = 1)\n",
    "det['app'].value_counts()\n",
    "counts = det['app'].value_counts()\n",
    "det = det[~det['app'].isin(counts[counts < 100].index)]\n",
    "det['app'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current.drop_duplicates('src_ip', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = data.groupby('src_ip').count()\n",
    "head.head()\n",
    "head.reset_index(inplace = True)\n",
    "val = head[['src_ip', 'app']]\n",
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = pd.merge(current, val, how = 'inner', on = 'src_ip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current.rename(columns={'app':'daily_frequency'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = data[['src_ip', 'ssh_password']]\n",
    "new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data[['src_ip', 'ssh_password']]\n",
    "new_data.head()\n",
    "new_data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['length_password'] = new_data['ssh_password'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.drop(['ssh_password'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.drop_duplicates(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = pd.merge(current, new_data, how = 'outer', on = 'src_ip')\n",
    "current['length_password'].fillna(value = current['length_password'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current.drop_duplicates(inplace= True)\n",
    "current.drop_duplicates('src_ip', inplace= True)\n",
    "current.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_t = data.groupby('src_ip')['dest_port'].nunique()\n",
    "new_dat_t = dat_t.reset_index()\n",
    "new_dat_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current = pd.merge(new_dat_t, current, how = 'inner', on = 'src_ip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current['dest_port'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current.rename(columns={'dest_port':'dest_port_number'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current['length_command'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current['length_password'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current['length_username'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA - all honeypots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was my first attempt on Principal Component Analysis. PCA is a statistical procedure that uses orthogonal transformation to convert many possibly correlated numerical variables into a specified number of linearly uncorrelated variables. These variables are called principal components. In this analysis I perform PCA on data from all the honeypots, and my target is signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neccessary libraries for PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create a target in order to perform PCA\n",
    "# Find most common signature occuring for each ip\n",
    "signature = data[['src_ip', 'signature']]\n",
    "signature_grouped = signature.groupby('src_ip', as_index = False).agg({'signature':lambda x: x.tolist()})\n",
    "signature_grouped['signature'] = signature_grouped['signature'].apply(most_common)\n",
    "signature_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column named target which is basically a code for each signature\n",
    "# (PCA only accepts numerical values)\n",
    "\n",
    "signature_grouped['signature'].unique()\n",
    "def target(row):\n",
    "    if row == 'Connection to Honeypot':\n",
    "        return 1\n",
    "    elif row == 'SSH login attempted on cowrie honeypot':\n",
    "        return 2\n",
    "    elif row == 'SSH session on cowrie honeypot':\n",
    "        return 3\n",
    "    elif row == 'command attempted on cowrie honeypot':\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "signature_grouped['target'] = signature_grouped['signature'].apply(target) \n",
    "signature_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = pd.merge(signature_grouped, current, how = 'inner', on = 'src_ip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = current.drop(['signature'], axis = 1)\n",
    "current['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale your data\n",
    "scaler = StandardScaler()\n",
    "current_pca_all = current.drop([\"src_ip\"], axis = 1)\n",
    "scaler.fit(current_pca_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = scaler.transform(current_pca_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your dimensions\n",
    "pca = PCA(n_components=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the scaled data\n",
    "pca.fit(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pca = pca.transform(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your components against each other\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.scatter(x_pca[:,0],x_pca[:,1],c=current['target'], cmap='plasma')\n",
    "plt.title('Principal Component Analysis', fontsize = 15)\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "#plt.xlim(-1.5, 2.5)\n",
    "#plt.ylim(-5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is neccessary to view the correlation of individual components to one another\n",
    "pca.components_\n",
    "sns.heatmap(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA - Dionaea honeypot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realised that performing PCA on data from all the honeypots was not ideal because some of the data gathered only apply to a specific honeypot and no to the other. Therefore, we focused on dionaea honeypot and identifing components there. To do that, we first found the most common honeypot for each ip, and the target used was again signature. Interestingly enough, most of the signatures are of the same kind, and there is a distinct cluster forming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most common honeypot per ip\n",
    "app = data[['src_ip', 'app']]\n",
    "app_grouped = app.groupby('src_ip', as_index = False).agg({'app':lambda x: x.tolist()})\n",
    "app_grouped['app'] = app_grouped['app'].apply(most_common)\n",
    "app_grouped.head()\n",
    "app_grouped['app'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dionaea_grouped = app_grouped.loc[app_grouped['app'] == \"dionaea\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dionaea_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dionaea = pd.merge(dionaea_grouped, current, how = 'inner', on = 'src_ip')\n",
    "current_dionaea.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "current_pca_dionaea = current_dionaea.drop([\"src_ip\", \"app\"], axis = 1)\n",
    "current_pca_dionaea.head()\n",
    "scaler.fit(current_pca_dionaea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = scaler.transform(current_pca_dionaea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pca = pca.transform(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "plt.scatter(x_pca[:,0],x_pca[:,1],c=current_dionaea['target'], cmap='plasma')\n",
    "plt.title('Principal Component Analysis', fontsize = 15)\n",
    "\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.xlim(-1.5, 5)\n",
    "plt.ylim(-2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_\n",
    "sns.heatmap(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pca.explained_variance_ratio_[0:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - random danger data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neccessary Random Forest libabries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "rf_data = current.copy()\n",
    "rf_data = rf_data[['src_ip','mean_time_difference','sd_time_difference','sensor_number','length_password']]\n",
    "\n",
    "#adding new column (random data) which should be our output. Potentionally we will have this column\n",
    "rf_data['danger'] = random.choices(population = [1,0],weights = [0.75,0.25], k = len(rf_data))\n",
    "train = rf_data[:8000]\n",
    "test = rf_data[8000:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_vars = ['mean_time_difference','sd_time_difference','sensor_number','length_password']\n",
    "X, y = train[predictor_vars], train.danger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRandom = RandomForestClassifier(max_depth=25)\n",
    "modelRandomCV = cross_val_score(modelRandom,X,y,cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRandomCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRandom = RandomForestClassifier(max_depth=25)\n",
    "modelRandom.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = modelRandom.predict(test[predictor_vars])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['predictions'] = predictions\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(row):\n",
    "    if row['danger'] == row['predictions']:\n",
    "        return 'Correct'\n",
    "    else:\n",
    "        return 'Incorrect'\n",
    "test['check'] = test.apply(check,axis = 1)\n",
    "\n",
    "accuracy_total = len(test[test['check'] == 'Correct'])/len(test)\n",
    "accuracy_total\n",
    "#Hard to predict random data. Hence, low accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "danger = test[test['danger'] == 1]\n",
    "danger_accuracy = len(danger[danger['check'] == 'Correct'])/len(danger)\n",
    "danger_accuracy\n",
    "#As we said, since data are random it's hard to predict outcomes. Especially for \"danger\"(since \"danger\" label is only about 25% of our training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise classical Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "CM = confusion_matrix(test.danger, predictions)\n",
    "print(CM)\n",
    "\n",
    "# Visualize it as a heatmap\n",
    "import seaborn\n",
    "seaborn.heatmap(CM)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_curve, auc\n",
    "# fpr, tpr, _ = roc_curve(test.danger,predictions)\n",
    "\n",
    "# plt.clf()\n",
    "# plt.plot(fpr, tpr)\n",
    "\n",
    "# plt.xlim([-0.05, 1.05])\n",
    "# plt.ylim([-0.05, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver operating characteristic example')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {\"tags\": \"cloud\"}\n",
    "data.fillna(values, inplace = True)\n",
    "data['tags'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags(row):\n",
    "    if row == 'localnet,durham,honeynet' or row =='localnet,durham,sciencedmz' or row == 'localnet,durham':\n",
    "        return 0\n",
    "    if row == 'cloud':\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['new_tags'] = data['tags'].apply(tags) \n",
    "data['new_tags'].value_counts()\n",
    "\n",
    "new_data = data[['src_ip', 'new_tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rftags = pd.merge(current, new_data, how = \"inner\", on = \"src_ip\")\n",
    "rftags.drop_duplicates(subset = 'src_ip', inplace = True)\n",
    "rftags['new_tags'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = rftags[['mean_time_difference','sd_time_difference','sensor_number','length_password', 'length_username', \"length_command\", 'daily_frequency']]\n",
    "y = rftags['new_tags']\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTags = RandomForestClassifier(max_depth = 25)\n",
    "modelTags.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_tags = modelTags.predict(X_test)\n",
    "predictions_tags\n",
    "modelTags.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualise classical Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "CM = confusion_matrix(y_test, predictions_tags)\n",
    "print(CM)\n",
    "\n",
    "# Visualize it as a heatmap\n",
    "import seaborn\n",
    "seaborn.heatmap(CM)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,predictions_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
